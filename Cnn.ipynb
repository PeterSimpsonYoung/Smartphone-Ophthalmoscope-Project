{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import cv\n",
    "import subprocess as sp\n",
    "import os\n",
    "import sys\n",
    "from matplotlib import pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "image_size = 100\n",
    "\n",
    "def rotate(image,degrees):\n",
    "    \n",
    "    (h, w) = image.shape[:2]\n",
    "    center = (w / 2, h / 2)\n",
    "\n",
    "    # rotate the image by 180 degrees\n",
    "    M = cv2.getRotationMatrix2D(center, degrees, 1.0)\n",
    "    rotated = cv2.warpAffine(image, M, (w, h))\n",
    "    return rotated\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469\n",
      "400\n",
      "28\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread('/Users/fair/Desktop/MECH3921/IBM/Training/Glaucoma Images/Im256.jpg')\n",
    "cv2.destroyAllWindows()\n",
    "cv2.imshow('original',img)\n",
    "new_img = cv2.resize(img,(28,28))\n",
    "print np.size(img,0)\n",
    "print np.size(img,1)\n",
    "print np.size(new_img,0)\n",
    "print np.size(new_img,1)\n",
    "cv2.imshow('resized img',img)\n",
    "img = cv2.resize(img,(image_size,image_size))\n",
    "cv2.imshow('image',img)\n",
    "##cv2.imshow('frame',new_img)\n",
    "rotate(img,90)\n",
    "cv2.imshow('rotate',rotate(img,90))\n",
    "cv2.destroyAllWindows()\n",
    "##cv2.destroyWindow('image')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glaucomaData = []\n",
    "glaucomaLabels = []\n",
    "counter = 0\n",
    "for i in os.listdir(\"/Users/fair/Desktop/MECH3921/IBM/Training/Glaucoma Images\"):\n",
    "    if i.endswith(\".jpg\"): \n",
    "        \n",
    "        \n",
    "        image = cv2.imread(\"/Users/fair/Desktop/MECH3921/IBM/Training/Glaucoma Images/\" + i)\n",
    "        \n",
    "        new_image = cv2.resize(image,(image_size,image_size))\n",
    "        \n",
    "        glaucomaData.append(new_image)\n",
    "        glaucomaLabels.append([0,1])\n",
    "        \n",
    "        angle = 90\n",
    "        while angle < 360:\n",
    "            \n",
    "            image = rotate(new_image,angle)\n",
    "            \n",
    "            glaucomaData.append(image)\n",
    "            glaucomaLabels.append([0,1])\n",
    "            angle = angle + 90\n",
    "    \n",
    "        #cv2.imshow('frame',image)\n",
    "        #print np.size(image,0)\n",
    "        #print np.size(image,1)\n",
    "        #print np.size(image,2)\n",
    "       \n",
    "        counter = counter + 1\n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "healthyData = []\n",
    "healthyLabels = []\n",
    "counter = 0\n",
    "for i in os.listdir(\"/Users/fair/Desktop/MECH3921/IBM/Training/Normal\"):\n",
    "    if i.endswith(\".jpg\"): \n",
    "        \n",
    "        \n",
    "        image = cv2.imread(\"/Users/fair/Desktop/MECH3921/IBM/Training/Normal/\" + i)\n",
    "        new_image = cv2.resize(image,(image_size,image_size))\n",
    "        \n",
    "        healthyData.append(new_image)\n",
    "        healthyLabels.append([1,0])\n",
    "        \n",
    "        angle = 90\n",
    "        while angle < 360:\n",
    "            \n",
    "            image = rotate(new_image,angle)\n",
    "            \n",
    "            healthyData.append(image)\n",
    "            healthyLabels.append([1,0])\n",
    "            angle = angle + 90\n",
    "\n",
    "        #cv2.imshow('frame',image)\n",
    "        #print np.size(image,0)\n",
    "        #print np.size(image,1)\n",
    "        #print np.size(image,2)\n",
    "        counter = counter + 1\n",
    "        continue\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of glaucoma images is 800\n",
      "number of healthy images is 828\n",
      "dimention of healthy images is (828, 100, 100, 3)\n",
      "dimention of henlthy labels is (828, 2)\n"
     ]
    }
   ],
   "source": [
    "print \"number of glaucoma images is\",len(glaucomaData)\n",
    "print \"number of healthy images is\",len(healthyData)\n",
    "print \"dimention of healthy images is\", np.asarray(healthyData).shape\n",
    "print \"dimention of henlthy labels is\", np.asarray(healthyLabels).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimention of labels after concatenating is (1628, 2)\n",
      "dimention of data after concatenating is (1628, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "# Join two set of data and convert type of data to float32\n",
    "full_data = np.concatenate((glaucomaData,healthyData),0).astype(np.float32)\n",
    "full_labels = np.concatenate((glaucomaLabels,healthyLabels),0).astype(np.float32)\n",
    "print \"dimention of labels after concatenating is\" , full_labels.shape\n",
    "print \"dimention of data after concatenating is\" , full_data.shape\n",
    "# print full_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Always create same random array?\n",
    "np.random.seed(133)\n",
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "full_data,full_labels = randomize(full_data,full_labels)\n",
    "\n",
    "# What does this do?\n",
    "def shuffle_in_unison_inplace(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "#full_data, full_labels = shuffle_in_unison_inplace(full_data,full_labels)\n",
    "\n",
    "# Calculate the accuracy by comparing prediction and labels, [1,0] or [0,1]\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide dataset into 2, one for training, the other for validation.\n",
    "# But why 1600:1820? Should be 1600:1628?\n",
    "train_dataset = full_data[0:1600]\n",
    "train_labels = full_labels[0:1600]\n",
    "valid_dataset = full_data[1600:1820]\n",
    "valid_labels = full_labels[1600:1820]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################### Lets just be cheap and try it at 28*28.\n",
    "\n",
    "# Initialize the variables for Cnn\n",
    "\n",
    "# two classes: glaucoma and health\n",
    "num_labels = 2\n",
    "# 3 channels for each image\n",
    "num_channels = 3 # grayscale\n",
    "\n",
    "batch_size = 10\n",
    "patch_size = 7\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "num_steps = 180\n",
    "# 1820 /5 is 364"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  #tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "  layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "  layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "  #the reason its /4 is because we've max pooled twice \n",
    "    #halved twice (i.e /4)\n",
    "  layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size / 4 * image_size / 4 * depth, num_hidden], stddev=0.1))\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "  layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "  layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "  # the way conv2d works is data is the input, layer1weights is the filter, [1,strides,strides,1]\n",
    "    # is the strides throughtout the dimensions, first and 4th must be the same in 4D,  1. \n",
    "  def model(data):\n",
    "    conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    pool = tf.nn.max_pool(hidden, [1,2,2,1], [1,2,2,1], padding = 'SAME')\n",
    "    conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    pool = tf.nn.max_pool(hidden, [1,2,2,1], [1,2,2,1], padding = 'SAME')\n",
    "    shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "    return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "  logits = model(tf_train_dataset)\n",
    "  probabilities = tf.nn.softmax(logits)\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.03).minimize(loss)\n",
    "  optimizer = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    " # test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 209.489\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 64.3%\n",
      "Minibatch loss at step 10 : 147.62\n",
      "Minibatch accuracy: 40.0%\n",
      "Validation accuracy: 53.6%\n",
      "Minibatch loss at step 20 : 55.7034\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 57.1%\n",
      "Minibatch loss at step 30 : 13.3276\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 40 : 25.7165\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 71.4%\n",
      "Minibatch loss at step 50 : 16.6312\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 60 : 56.9417\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 70 : 18.1315\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 80 : 40.6252\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 90 : 34.1865\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 57.1%\n",
      "Minibatch loss at step 100 : 106.07\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 71.4%\n",
      "Minibatch loss at step 110 : 2.69581\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 75.0%\n",
      "Minibatch loss at step 120 : 52.4266\n",
      "Minibatch accuracy: 30.0%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 130 : 21.8669\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 140 : 23.3139\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 150 : 12.7208\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 160 : 33.5378\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 170 : 20.8113\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 85.7%\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  saver = tf.train.Saver()\n",
    "  print \"Initialized\"\n",
    "  for step in xrange(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict)\n",
    "    if (step % 10 == 0):\n",
    "      print \"Minibatch loss at step\", step, \":\", l\n",
    "      print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "      print \"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels)\n",
    "      #print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)\n",
    "    # Save variables to .ckpt file\n",
    "    saver.save(session, \"trained_variables.ckpt\")\n",
    "#  print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)\n",
    "session.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testData = []\n",
    "        \n",
    "for i in os.listdir(\"/Users/fair/Desktop/MECH3921/IBM/Input\"):\n",
    "    if i.endswith(\".jpg\"): \n",
    "        \n",
    "        image = cv2.imread(\"/Users/fair/Desktop/MECH3921/IBM/Input/\" + i)\n",
    "        \n",
    "        new_image = cv2.resize(image,(image_size,image_size))\n",
    "        \n",
    "        testData.append(new_image)\n",
    "        \n",
    "        continue\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "(10, 100, 100, 3)\n",
      "(100, 100, 3)\n",
      "[[  1.00000000e+00   0.00000000e+00]\n",
      " [  0.00000000e+00   1.00000000e+00]\n",
      " [  0.00000000e+00   1.00000000e+00]\n",
      " [  1.10471485e-21   1.00000000e+00]\n",
      " [  3.19388098e-13   1.00000000e+00]\n",
      " [  1.60810787e-25   1.00000000e+00]\n",
      " [  5.77335836e-15   1.00000000e+00]\n",
      " [  5.23759508e-12   1.00000000e+00]\n",
      " [  0.00000000e+00   1.00000000e+00]\n",
      " [  0.00000000e+00   1.00000000e+00]]\n",
      "['healthy', 'glaucoma', 'glaucoma', 'glaucoma', 'glaucoma', 'glaucoma', 'glaucoma', 'glaucoma', 'glaucoma', 'glaucoma']\n"
     ]
    }
   ],
   "source": [
    "### Evaluate some new data!\n",
    "with tf.Session(graph=graph) as sess:\n",
    "      saver = tf.train.Saver()\n",
    "      saver.restore(sess, \"trained_variables.ckpt\")\n",
    "      print \"Initialized\"\n",
    "      \n",
    "      testSet = np.asarray(testData).astype(np.float32)\n",
    "      tests = testSet[0:10, :, :, :]\n",
    "      print batch_data.shape\n",
    "     \n",
    "      print testSet[0, :, :, :].shape\n",
    "      feed_dict={tf_train_dataset:tests}\n",
    "      predictions = sess.run(probabilities,\n",
    "                       feed_dict)\n",
    "      \n",
    "      print predictions\n",
    "      classifications = []\n",
    "      for i in xrange(len(predictions)):\n",
    "            classNo = np.argmax(predictions,1)[i]\n",
    "            if classNo == 1:\n",
    "                classifications.append(\"glaucoma\")\n",
    "            elif classNo ==0:\n",
    "                classifications.append(\"healthy\")\n",
    "                \n",
    "      print classifications\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
